### Gradient Descent

**[Gradient descent](https://en.wikipedia.org/wiki/Gradient_descent)** is an iterative approach to determining a local minimum of a differentiable function. Steps should be repeatedly taken in the opposite direction of the gradient (or approximate gradient) of the function at the current point, since this is the direction of steepest descent. Everytime when the prediction is not correct, the gradient descent will help to correct the weight vector to the right direction for the next iteration.

<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Gradient_descent.svg/350px-Gradient_descent.svg.png" width="400" align = 'center'/>

Gradient Descent is an optimization algorithm for finding a local minimum of a differentiable function. Gradient descent is simply used in machine learning to find the values of a function's parameters (coefficients) that minimize a cost function as far as possible.

This folder contains introduction of gradient descent and its implementation. We will also use Fish dataset in the notebook for the gradient descent, and use it to implement regression.
