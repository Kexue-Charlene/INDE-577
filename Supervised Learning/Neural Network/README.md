### Multilayer Perceptron
Multilayer perceptron (MLP) is a class of feed forward neural network and sometimes refer to networks composed of multiple layers of perceptrons. 

There are at least three layers of nodes in MLP: an input layer, a hidden layer and an output layer. The MLP consists of three or more layers (an input and an output layer with one or more hidden layers) of nonlinearly-activating nodes. Since MLPs are fully connected, each node in one layer connects with a certain weight $w_{ij}$ to every node in the following layer.
For the layers after input layer, every node is a neuron that uses a nonlinear activation function. MLP model use backpropagation for training, and it is a supervised learning technique.

#### Activation function
If a multilayer perceptron has a linear activation function in all neurons, that is, a linear function that maps the weighted inputs to the output of each neuron, then linear algebra shows that any number of layers can be reduced to a two-layer input-output model. In MLPs some neurons use a nonlinear activation function that was developed to model the frequency of action potentials, or firing, of biological neurons.

The two historically common activation functions are both sigmoids, and are described by <img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/167e8b5c38130ec92a2771bc384658772f387d02" width="300"/> In recent developments of deep learning the rectifier linear unit (ReLU) is more frequently used as one of the possible ways to overcome the numerical problems related to the sigmoids. Here <img src="https://render.githubusercontent.com/render/math?math=y_{i}"> is the output of the ith node (neuron) and <img src="https://render.githubusercontent.com/render/math?math=v_{i}"> is the weighted sum of the input connections. 

#### Learning
Learning occurs in the perceptron by changing connection weights after each piece of data is processed, based on the amount of error in the output compared to the expected result. This is an example of supervised learning, and is carried out through backpropagation, a generalization of the least mean squares algorithm in the linear perceptron. 

In fitting a neural network, backpropagation computes the gradient of the loss function with respect to the weights of the network for a single inputâ€“output example, and does so efficiently, unlike a naive direct computation of the gradient with respect to each weight individually. This efficiency makes it feasible to use gradient methods for training multilayer networks, updating weights to minimize loss; gradient descent, or variants such as stochastic gradient descent, are commonly used. The backpropagation algorithm works by computing the gradient of the loss function with respect to each weight by the chain rule, computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule.

We can represent the degree of error in an output node j in the nth data point (training example) by <img src="https://render.githubusercontent.com/render/math?math={\displaystyle e_{j}(n)=d_{j}(n)-y_{j}(n)}">, where d is the target value and y is the value produced by the perceptron. The node weights can then be adjusted based on corrections that minimize the error in the entire output, given by <img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/41454c8f3507f945e99dc7e18e8225d1bb0830de" width="100"/>  Using gradient descent, the change in each weight is <img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e775e1fd516ec50eaf45344d5429657686c6985c" width="150"/> where <img src="https://render.githubusercontent.com/render/math?math={\displaystyle y_{i}}"> is the output of the previous neuron and <img src="https://render.githubusercontent.com/render/math?math={\displaystyle \eta }"> is the learning rate, which is selected to ensure that the weights quickly converge to a response, without oscillations.
