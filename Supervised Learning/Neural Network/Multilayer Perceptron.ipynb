{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jp2g_T-cltJ"
   },
   "source": [
    "## Multilayer Perceptron\n",
    "Multilayer perceptron (MLP) is a class of feed forward neural network and sometimes refer to networks composed of multiple layers of perceptrons. \n",
    "\n",
    "There are at least three layers of nodes in MLP: an input layer, a hidden layer and an output layer. The MLP consists of three or more layers (an input and an output layer with one or more hidden layers) of nonlinearly-activating nodes. Since MLPs are fully connected, each node in one layer connects with a certain weight $w_{ij}$ to every node in the following layer.\n",
    "For the layers after input layer, every node is a neuron that uses a nonlinear activation function. MLP model use backpropagation for training, and it is a supervised learning technique.\n",
    "\n",
    "#### Activation function\n",
    "If a multilayer perceptron has a linear activation function in all neurons, that is, a linear function that maps the weighted inputs to the output of each neuron, then linear algebra shows that any number of layers can be reduced to a two-layer input-output model. In MLPs some neurons use a nonlinear activation function that was developed to model the frequency of action potentials, or firing, of biological neurons.\n",
    "\n",
    "The two historically common activation functions are both sigmoids, and are described by <img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/167e8b5c38130ec92a2771bc384658772f387d02\" width=\"300\"/> In recent developments of deep learning the rectifier linear unit (ReLU) is more frequently used as one of the possible ways to overcome the numerical problems related to the sigmoids. Here $y_{i}$ is the output of the ${\\displaystyle i}$th node (neuron) and ${\\displaystyle v_{i}}$ is the weighted sum of the input connections. \n",
    "\n",
    "#### Learning\n",
    "Learning occurs in the perceptron by changing connection weights after each piece of data is processed, based on the amount of error in the output compared to the expected result. This is an example of supervised learning, and is carried out through backpropagation, a generalization of the least mean squares algorithm in the linear perceptron. \n",
    "\n",
    "In fitting a neural network, backpropagation computes the gradient of the loss function with respect to the weights of the network for a single inputâ€“output example, and does so efficiently, unlike a naive direct computation of the gradient with respect to each weight individually. This efficiency makes it feasible to use gradient methods for training multilayer networks, updating weights to minimize loss; gradient descent, or variants such as stochastic gradient descent, are commonly used. The backpropagation algorithm works by computing the gradient of the loss function with respect to each weight by the chain rule, computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule.\n",
    "\n",
    "We can represent the degree of error in an output node ${\\displaystyle j}$ in the ${\\displaystyle nth}$ data point (training example) by ${\\displaystyle e_{j}(n)=d_{j}(n)-y_{j}(n)}$, where ${\\displaystyle d}$ is the target value and ${\\displaystyle y}$ is the value produced by the perceptron. The node weights can then be adjusted based on corrections that minimize the error in the entire output, given by <img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/41454c8f3507f945e99dc7e18e8225d1bb0830de\" width=\"100\"/>  Using gradient descent, the change in each weight is <img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/e775e1fd516ec50eaf45344d5429657686c6985c\" width=\"150\"/> where ${\\displaystyle y_{i}}$ is the output of the previous neuron and ${\\displaystyle \\eta }$ is the learning rate, which is selected to ensure that the weights quickly converge to a response, without oscillations.\n",
    "\n",
    "The derivative to be calculated depends on the induced local field ${\\displaystyle v_{j}}$, which itself varies. It is easy to prove that for an output node this derivative can be simplified to <img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/056be9bc7c738ade1a15914654576d0de972594b\" width=\"150\"/> where ${\\displaystyle \\phi ^{\\prime }}$ is the derivative of the activation function described above, which itself does not vary. The analysis is more difficult for the change in weights to a hidden node, but it can be shown that the relevant derivative is <img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/a57fb40387f833ae8d731f78c04138ad2ce6890b\" width=\"225\"/> This depends on the change in weights of the ${\\displaystyle k}$ nodes, which represent the output layer. So to change the hidden layer weights, the output layer weights change according to the derivative of the activation function, and so this algorithm represents a backpropagation of the activation function.\n",
    "\n",
    "In class, we build a multilayered perceptron with a single input layer with $784$ input nodes, 2 hidden layers of arbitrary size, and $10$ output nodes. These layers will be denoted $L^0, L^1, L^2,$ and $L^{3}$, respectively. \n",
    "<img src=\"https://miro.medium.com/proxy/1*eloYEyFrblGHVZhU345PJw.jpeg\" width=\"500\"/>\n",
    "For $l = 1, 2, 3$, layer $l$ will have two phases:\n",
    "\n",
    "* The preactivation phase $z^l = W^la^{l-1} + b^l,$ \n",
    "* The postactivation phase $a^l = \\sigma(z^l).$ \n",
    "\n",
    "The preactivation phase consists of a weighted linear combination of postactivation values in the previous layer. The postactivation values consists of passing the preactivation value through a chosen activation function elementwise. For notational convience, we let $a^0 = x$, where $x$ is the current input data into our network. For our activation function, we will use the sigmoid function:\n",
    "\n",
    "* Sigmoid Function\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1+e^{-z}}.\n",
    "$$\n",
    "\n",
    "For our cost function, we will use the Mean Sqaure Error cost:\n",
    "$$\n",
    "C = C(W, b) = \\frac{1}{2}\\sum_{i=1}^n(a^i - y^i)^2.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "\n",
    "### In Class Example\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first example we build is a four layers included input layer network. Because we have 784 input data in the first layer, we have 784 neurons in the input layer.Also we have 60 nodes in the two hidden layers, and since we have 10 classes, the last layer will need to have 10 nodes. The number of nodes in the hidden layers can be whatever you want.  \n",
    "\n",
    "Our task is to classify handwritten digits from 0 to 9.  \n",
    "In order to do this we will need the following libraries:\n",
    "\n",
    "* [matplotlib](https://matplotlib.org/)\n",
    "* [tensorflow](https://www.tensorflow.org/)\n",
    "* [keras](https://keras.io/) We need to import keras to access the dataset because the dataset is in keras, and that is the only time we will be using keras in this notebook; we will not use other functions under keras to build the network.\n",
    "* [numpy](https://numpy.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries and dataset from keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset information\n",
    "The [MNIST dataset](https://keras.io/api/datasets/mnist/) consists of $70000$  28x28 grayscale images of hand written digits, $60000$ of which are typically used as labeled training examples, where the other $10000$ are used for testing your learning model on. The following picture represent a sample of some of the images.\n",
    "<img src=\"https://miro.medium.com/max/1168/1*2lSjt9YKJn9sxK7DSeGDyw.jpeg\" width=\"500\"/>\n",
    "\n",
    "\n",
    "\n",
    "Each image in the MNIST data set is stored as a matrix. And every pixel values range from 0 to 255\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the shape, we have 60000 matrix (pictures) with 28 by 28 pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the shape, we have 60000 pictures so 60000 numbers (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x15a9b193e50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_X[0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the range of train_X for each picture. Max is 225 min is 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 255)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(train_X),np.max(train_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scale each picture's pixel so that it prevents exploding during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_X/255\n",
    "test_X = test_X/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we want to flat X so that it changes from the matix (picture shape) to a list of number\n",
    "we now check the flatten function first and the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[0].flatten().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we want to reshape it into (784,1) : change to a vector with 784 rows and 1 col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[0].flatten().reshape(784, 1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remember y_train is just the label number corresponding to each picture.Aassign the right row corresponding to the label number to 1 to make one hot encoded, so we need to get into the number itself first instead of array, and then we can assign 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X will temp store flattened matrices\n",
    "X = []\n",
    "for x in train_X:\n",
    "  X.append(x.flatten().reshape(784, 1))\n",
    "\n",
    "# Y will temp store one-hot encoded label vectors\n",
    "Y = []\n",
    "for y in train_y:\n",
    "  temp_vec = np.zeros((10, 1))\n",
    "  temp_vec[y][0] = 1.0\n",
    "  Y.append(temp_vec)\n",
    "\n",
    "# Our data will be stored as a list of tuples. \n",
    "train_data = [p for p in zip(X, Y)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have 60000 rows and two other cols one for pixel value the other for one hot encoded label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\c\\AppData\\Local\\Temp/ipykernel_20328/2494387256.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  np.array(train_data).shape\n"
     ]
    }
   ],
   "source": [
    "p = train_data[0]\n",
    "np.array(train_data).shape \n",
    "p[0].shape\n",
    "print(p[1])\n",
    "print(train_y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the same operation for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for x in test_X:\n",
    "  X.append(x.flatten().reshape(784, 1))\n",
    "\n",
    "Y = []\n",
    "for y in test_y:\n",
    "  temp_vec = np.zeros((10, 1))\n",
    "  temp_vec[y][0] = 1.0\n",
    "  Y.append(temp_vec)\n",
    "\n",
    "test_data = [p for p in zip(X, Y)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define activation function (sigmoid), derivative of sigmoid, and MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "  return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "# get the derivative of sigmoid\n",
    "def sigmoid_prime(z):\n",
    "  return sigmoid(z)*(1.0-sigmoid(z))\n",
    "\n",
    "# a is the predicted probabilty for each possible class label\n",
    "def mse(a, y):\n",
    "  return .5*sum((a[i]-y[i])**2 for i in range(10))[0]\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup initializers weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(layers = [784, 60, 60, 10]):\n",
    "  W = [[0.0]]\n",
    "  B = [[0.0]]\n",
    "  # weight dim (60,784) (60,60) (10,60);  bias dim (60,1) (60,1) (10,1)\n",
    "  # range(1, len(layers) not included len(layers)\n",
    "  for i in range(1, len(layers)):\n",
    "    w_temp = np.random.randn(layers[i], layers[i-1])*np.sqrt(2/layers[i-1])\n",
    "    b_temp = np.random.randn(layers[i], 1)*np.sqrt(2/layers[i-1])\n",
    "\n",
    "    W.append(w_temp)\n",
    "    B.append(b_temp)\n",
    "  return W, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, B = initialize_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a0 = x\n",
    "z1 = (W[1] @ a0) + B[1]\n",
    "a1 = sigmoid(z1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fist layer output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2nd layer output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 1)\n"
     ]
    }
   ],
   "source": [
    "z2 = (W[2] @ a1) + B[2]\n",
    "a2 = sigmoid(z2)\n",
    "print(a2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "last layer output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "z3 = (W[3] @ a2) + B[3]\n",
    "a3 = sigmoid(z3)\n",
    "print(a3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record hidden layers and output layer output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, B = initialize_weights(layers=[784, 60, 60, 10])\n",
    "x, y = train_data[0]\n",
    "Z = [[0.0]]\n",
    "A = [x]\n",
    "L = len(B)\n",
    "for i in range(1, L):\n",
    "  z = (W[i] @ A[i-1]) + B[i]\n",
    "  a = sigmoid(z)\n",
    "\n",
    "  Z.append(z)\n",
    "  A.append(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write and record Delta which is the term that remains the same for each gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas = dict()\n",
    "delta_last = (A[-1] - y)*sigmoid_prime(Z[-1]) # (predict-ture)*sigmoid prime\n",
    "deltas[L-1] = delta_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.14585446],\n",
       "       [ 0.1323334 ],\n",
       "       [ 0.10667142],\n",
       "       [ 0.12617306],\n",
       "       [ 0.11107781],\n",
       "       [-0.07099325],\n",
       "       [ 0.10562972],\n",
       "       [ 0.13738977],\n",
       "       [ 0.14814336],\n",
       "       [ 0.14073522]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltas[L-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "last layer delta*corresponding weight*sigmoid prime for the layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in range(L-2, 0, -1):\n",
    "  deltas[l] = (W[l+1].T @ deltas[l+1])*sigmoid_prime(Z[l]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set learning rate alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 4):\n",
    "  W[i] = W[i] - alpha*deltas[i]@A[i-1].T\n",
    "  B[i] = B[i] - alpha*deltas[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define feedforward process and delta terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(W, B, p, predict_vector = False):\n",
    "  Z =[[0.0]]\n",
    "  A = [p[0]] #the first train_X\n",
    "  L = len(W)\n",
    "  for i in range(1, L):\n",
    "    z = (W[i] @ A[i-1]) + B[i]\n",
    "    a = sigmoid(z)\n",
    "\n",
    "    Z.append(z)\n",
    "    A.append(a)\n",
    "\n",
    "  if predict_vector == True:\n",
    "    return A[-1]\n",
    "  else:\n",
    "    return Z, A\n",
    "\n",
    "def deltas_dict(W, B, p):\n",
    "  Z, A = forward_pass(W, B, p)\n",
    "  L = len(W)\n",
    "  deltas = dict()\n",
    "  deltas[L-1] = (A[-1] - p[1])*sigmoid_prime(Z[-1])\n",
    "  for l in range(L-2, 0, -1):\n",
    "    deltas[l] = (W[l+1].T @ deltas[l+1]) * sigmoid_prime(Z[l])\n",
    "\n",
    "  return A, deltas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(W, B, data):\n",
    "  c = 0.0\n",
    "  for p in data:\n",
    "    a = forward_pass(W, B, p, predict_vector=True)\n",
    "    c += mse(a, p[1])\n",
    "  return c/len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See initial cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost = 1.499725656571191\n"
     ]
    }
   ],
   "source": [
    "W, B = initialize_weights()\n",
    "print(f\"Initial Cost = {MSE(W, B, train_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See if our prediction is correct. No it is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Value = 1\n",
      "Actual Value = 6\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = np.random.randint(0, len(test_X))\n",
    "prediction = np.argmax(forward_pass(W, B, test_data[i], predict_vector=True))\n",
    "print(f\"Predicted Value = {prediction}\")\n",
    "print(f\"Actual Value = {test_y[i]}\")\n",
    "plt.imshow(test_X[i], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(W, B, data, alpha = 0.04, epochs = 3):\n",
    "  L = len(W)\n",
    "  print(f\"Initial Cost = {MSE(W, B, data)}\")\n",
    "  for k in range(epochs):\n",
    "    for p in data:\n",
    "      A, deltas = deltas_dict(W, B, p)\n",
    "      for i in range(1, L):\n",
    "        W[i] = W[i] - alpha*deltas[i]@A[i-1].T\n",
    "        B[i] = B[i] - alpha*deltas[i]\n",
    "    print(f\"Cost at step {k} = {MSE(W, B, data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost = 1.499725656571191\n",
      "Cost at step 0 = 0.0747808289089981\n",
      "Cost at step 1 = 0.053160474582893524\n",
      "Cost at step 2 = 0.04223445399843722\n"
     ]
    }
   ],
   "source": [
    "stochastic_gradient_descent(W, B, train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we see if our prediction is correct. Yes it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Value = 1\n",
      "Actual Value = 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = np.random.randint(0, len(test_X))\n",
    "prediction = np.argmax(forward_pass(W, B, test_data[i], predict_vector=True))\n",
    "print(f\"Predicted Value = {prediction}\")\n",
    "print(f\"Actual Value = {test_y[i]}\")\n",
    "plt.imshow(test_X[i], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Gradient Descent, Stochastic Gradient Descent, and Mini Batch Gradient Descent\n",
    "* **Batch Gradient Descent**\n",
    "In Batch Gradient Descent, all the training data is taken into consideration to take a single step. We take the average of the gradients of all the training examples and then use that mean gradient to update our parameters. So thatâ€™s just one step of gradient descent in one epoch.    \n",
    "\n",
    "\n",
    "* **Stochastic Gradient Descent**\n",
    "In Stochastic Gradient Descent (SGD), we consider just one example at a time to take a single step. We do the following steps in one epoch for SGD:\n",
    "1. Take an example\n",
    "2. Feed it to Neural Network\n",
    "3. Calculate itâ€™s gradient\n",
    "4. Use the gradient we calculated in step 3 to update the weights\n",
    "5. Repeat steps 1â€“4 for all the examples in training dataset  \n",
    "Since we are considering just one example at a time the cost will fluctuate over the training examples and it will not necessarily decrease. But in the long run, you will see the cost decreasing with fluctuations. SGD can be used for larger datasets. It converges faster when the dataset is large as it causes updates to the parameters more frequently.   \n",
    "\n",
    "Batch Gradient Descent can be used for smoother curves. SGD can be used when the dataset is large. Batch Gradient Descent converges directly to minima. SGD converges faster for larger datasets.\n",
    "\n",
    "* **Mini Batch Gradient Descent**\n",
    "We use a batch of a fixed number of training examples which is less than the actual dataset and call it a mini-batch. Doing this helps us achieve the advantages of both the former variants we saw. So, after creating the mini-batches of fixed size, we do the following steps in one epoch:\n",
    "1. Pick a mini-batch\n",
    "2. Feed it to Neural Network\n",
    "3. Calculate the mean gradient of the mini-batch\n",
    "4. Use the mean gradient we calculated in step 3 to update the weights\n",
    "5. Repeat steps 1â€“4 for the mini-batches we created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we put everything together in a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerPerceptron():\n",
    "  \n",
    "  def __init__(self, layers = [784, 60, 60, 10]):\n",
    "    self.layers = layers\n",
    "    self.L = len(self.layers)\n",
    "    self.W =[[0.0]]\n",
    "    self.B = [[0.0]]\n",
    "    for i in range(1, self.L):\n",
    "      w_temp = np.random.randn(self.layers[i], self.layers[i-1])*np.sqrt(2/self.layers[i-1])\n",
    "      b_temp = np.random.randn(self.layers[i], 1)*np.sqrt(2/self.layers[i-1])\n",
    "\n",
    "      self.W.append(w_temp)\n",
    "      self.B.append(b_temp)\n",
    "\n",
    "  def reset_weights(self, layers = [784, 60, 60, 10]):\n",
    "    self.layers = layers\n",
    "    self.L = len(self.layers)\n",
    "    self.W = [[0.0]]\n",
    "    self.B = [[0.0]]\n",
    "    for i in range(1, self.L):\n",
    "      w_temp = np.random.randn(self.layers[i], self.layers[i-1])*np.sqrt(2/self.layers[i-1])\n",
    "      b_temp = np.random.randn(self.layers[i], 1)*np.sqrt(2/self.layers[i-1])\n",
    "\n",
    "      self.W.append(w_temp)\n",
    "      self.B.append(b_temp)\n",
    "\n",
    "\n",
    "  def forward_pass(self, p, predict_vector = False):\n",
    "    Z =[[0.0]]\n",
    "    A = [p[0]]\n",
    "    for i in range(1, self.L):\n",
    "      z = (self.W[i] @ A[i-1]) + self.B[i]\n",
    "      a = sigmoid(z)\n",
    "      Z.append(z)\n",
    "      A.append(a)\n",
    "\n",
    "    if predict_vector == True:\n",
    "      return A[-1]\n",
    "    else:\n",
    "      return Z, A\n",
    "\n",
    "  def MSE(self, data):\n",
    "    c = 0.0\n",
    "    for p in data:\n",
    "      a = self.forward_pass(p, predict_vector=True)\n",
    "      c += mse(a, p[1])\n",
    "    return c/len(data)\n",
    "\n",
    "  def deltas_dict(self, p):\n",
    "    Z, A = self.forward_pass(p)\n",
    "    deltas = dict()\n",
    "    deltas[self.L-1] = (A[-1] - p[1])*sigmoid_prime(Z[-1])\n",
    "    for l in range(self.L-2, 0, -1):\n",
    "      deltas[l] = (self.W[l+1].T @ deltas[l+1]) * sigmoid_prime(Z[l])\n",
    "\n",
    "    return A, deltas\n",
    "\n",
    "  def stochastic_gradient_descent(self, data, alpha = 0.04, epochs = 3):\n",
    "    print(f\"Initial Cost = {self.MSE(data)}\")\n",
    "    for k in range(epochs):\n",
    "      for p in data:\n",
    "        A, deltas = self.deltas_dict(p)\n",
    "        for i in range(1, self.L):\n",
    "          self.W[i] = self.W[i] - alpha*deltas[i]@A[i-1].T\n",
    "          self.B[i] = self.B[i] - alpha*deltas[i]\n",
    "    print(f\"{k} Cost = {self.MSE(data)}\")\n",
    "\n",
    "\n",
    "  def mini_batch_gradient_descent(self, data, batch_size = 15, alpha = 0.04, epochs = 3):\n",
    "    print(f\"Initial Cost = {self.MSE(data)}\")\n",
    "    data_length = len(data)\n",
    "    for k in range(epochs):\n",
    "      for j in range(0, data_length-batch_size, batch_size):\n",
    "        delta_list = []\n",
    "        A_list = []\n",
    "        for p in data[j:j+batch_size]:\n",
    "          A, deltas = self.deltas_dict(p)\n",
    "          delta_list.append(deltas)\n",
    "          A_list.append(A)\n",
    "\n",
    "        for i in range(1, self.L):\n",
    "          self.W[i] = self.W[i] - (alpha/batch_size)*sum(da[0][i]@da[1][i-1].T for da in zip(delta_list, A_list))\n",
    "          self.B[i] = self.B[i] - (alpha/batch_size)*sum(deltas[i] for deltas in delta_list)\n",
    "    print(f\"{k} Cost = {self.MSE(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup 60 nodes in both hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MultilayerPerceptron(layers=[784, 60, 60, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost = 1.3860286281030807\n",
      "2 Cost = 0.041197465999243454\n"
     ]
    }
   ],
   "source": [
    "net.stochastic_gradient_descent(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost = 0.041197465999243454\n",
      "2 Cost = 0.03695976783035487\n"
     ]
    }
   ],
   "source": [
    "net.mini_batch_gradient_descent(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.039466441109262"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.MSE(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase node in hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MultilayerPerceptron(layers=[784, 120, 120, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost = 1.165598656494287\n",
      "2 Cost = 0.04411404028937365\n"
     ]
    }
   ],
   "source": [
    "net.stochastic_gradient_descent(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost = 0.04411404028937365\n",
      "2 Cost = 0.03910231769848193\n"
     ]
    }
   ],
   "source": [
    "net.mini_batch_gradient_descent(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04068733847909721"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.MSE(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini batch gradient descent has smaller loss, and there are not too much different after we increase the number of nodes in the hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------\n",
    "\n",
    "### Implementation on Different Dataset\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Information\n",
    "The dataset we will be using is [Fashion MNIST dataset](https://keras.io/api/datasets/fashion_mnist/). We can load the dataset from keras.\n",
    "\n",
    "* The dataset has 60,000 training data with 28x28 grayscale images\n",
    "* 10,000 test images\n",
    "* Data are labeled over 10 categories\n",
    "* This pixel depth allows 255 different intensities, with 0 being black and 255 being white\n",
    "* The classes are: T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, and Ankle boot with labels 0 to 9 respectively. \n",
    "\n",
    "The libraries we will be using are:\n",
    "* [matplotlib](https://matplotlib.org/)\n",
    "* [tensorflow](https://www.tensorflow.org/)\n",
    "* [keras](https://keras.io/) We need to import keras to access the dataset because the dataset is in keras, and that is the only time we will be using keras in this notebook; we will not use other functions under keras to build the network.\n",
    "* [numpy](https://numpy.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load libraries and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras # to load the data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from keras.datasets import fashion_mnist\n",
    "\n",
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "(train_X, train_y), (test_X, test_y) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the shape of train_X. We have 60000 training pictures each with 28 by 28 pixel and grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DLMr5GoLoXOX",
    "outputId": "12c53594-c540-407f-affb-43bc0722ac44"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the shape of train_X. We have 60000 training pictures each with 28 by 28 pixel and grayscale\n",
    "train_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the first training image dimension: it is a natrix with 28 by 28 dim, and it makes sense because it has 28 by 28 pixel\n",
    "train_X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NnmX7oVroqnB",
    "outputId": "6f49310f-f62b-4daf-f247-d31ea247b199"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the first training image dimension: it is a natrix with 28 by 28 dim, and it makes sense because it has 28 by 28 pixel\n",
    "train_X[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the first label for train_y: the first picture has label 9, and so it correspond to Ankle boot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_2Bn_HlOouLx",
    "outputId": "faab5c9a-278b-4568-da0c-a77001786f50"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the first label for train_y: the first picture has label 9, and so it correspond to Ankle boot\n",
    "train_y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize the first image by putting the pixel together, and it is an Ankle boot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "U500A41Goxyw",
    "outputId": "81c307bd-afea-44d9-f9b3-261d2323f431"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x15b1557e8e0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualize the first image by putting the pixel together, and it is an Ankle boot\n",
    "plt.imshow(train_X[0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the train_X range: it has min=0 and max=255. Because grayscale image has 0 to 255 values for each pixel to represent the intension of brightness, 0 represents black and 255 represents white."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MNQ2bm7oo2mx",
    "outputId": "d7efdf74-2381-45d9-b108-1aeafff4c06d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 255)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the train_X range: it has min=0 and max=255. Because grayscale image has 0 to 255 values for each pixel to represent the intension of brightness, 0 represents black and 255 represents white.\n",
    "np.min(train_X),np.max(train_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we want to scale the X first by their brightness intension to prevent explosion during training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "3mAE2bI2o-BB"
   },
   "outputs": [],
   "source": [
    "# we want to scale the X first by their brightness intension to prevent explosion during training \n",
    "train_X = train_X/255\n",
    "test_X = test_X/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "also check the dim for the first test_X, and it is also 28 by 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LCz44XiPpQLx",
    "outputId": "336d459e-285d-4aa4-e6c8-fec2fb4dc6e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# also check the dim for the first test_X, and it is also 28 by 28\n",
    "test_X[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to change the matrix shape to a vector to feed into the network, so let's try to flatten the first train_X and see its dim, so we know what does it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2uI2S2hmhU_N",
    "outputId": "c11d3517-51f1-4a60-c29b-510247fda273"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We want to change the matrix shape to a vector to feed into the network, so let's try to flatten the first train_X and see its dim, so we know what does it looks like\n",
    "train_X[0].flatten().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next we reshape it to (784,1) which is a vector (28x28=784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jT8DsxC-h9Ef",
    "outputId": "f435906a-2a03-4851-a936-02da83406e3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 1)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#next we reshape it to (784,1) which is a vector (28x28=784)\n",
    "train_X[0].flatten().reshape(784, 1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to flatten the X matrix. Make a list first and then store the flatten vetors into it. And we also make the y to one hot encoded label vectors; we make a list and store the vectors into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "FW-IrQORpSLw"
   },
   "outputs": [],
   "source": [
    "# Now we are ready to flatten the X matrix. Make a list first and then store the flatten vetors into it. And we also make the y to one hot encoded label vectors; we make a list and store the vectors into it.\n",
    "X = []\n",
    "for x in train_X:\n",
    "  X.append(x.flatten().reshape(784, 1))\n",
    "\n",
    "# Y will temp store one-hot encoded label vectors\n",
    "Y = []\n",
    "for y in train_y:\n",
    "  temp_vec = np.zeros((10, 1))\n",
    "  temp_vec[y][0] = 1.0\n",
    "  Y.append(temp_vec)\n",
    "\n",
    "# Our data will be stored as a list of tuples. \n",
    "train_data = [p for p in zip(X, Y)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set p as the first train data including X and y. p[0] is X, and p[1] is y all for training  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "90JMH_S57bih"
   },
   "outputs": [],
   "source": [
    "#set p as the first train data including X and y. p[0] is X, and p[1] is y all for training  \n",
    "p = train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check y value for the first train data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v8eOrEF2jibM",
    "outputId": "5693e0fe-d9d2-4a13-c2ee-527d0a4bce66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "# check y value for the first train data \n",
    "print(p[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check X dim for the first train data: it is a vector with 784 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TwLjYwge72PP",
    "outputId": "9a4fd112-420b-4798-d9bf-fb01b7381945"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 1)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check X dim for the first train data: it is a vector with 784 rows\n",
    "p[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check dim of train data, 60000 input with X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fr9D2uMdjxAd",
    "outputId": "625e96e9-597e-4756-c4c0-44860e03c47b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\c\\AppData\\Local\\Temp/ipykernel_20328/999522853.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  np.array(train_data).shape\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(60000, 2)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check dim of train data, 60000 input with X and y\n",
    "np.array(train_data).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the same method to deal with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "KcZIVltbp_vv"
   },
   "outputs": [],
   "source": [
    "# the same method to deal with test data\n",
    "X = []\n",
    "for x in test_X:\n",
    "  X.append(x.flatten().reshape(784, 1))\n",
    "\n",
    "Y = []\n",
    "for y in test_y:\n",
    "  temp_vec = np.zeros((10, 1))\n",
    "  temp_vec[y][0] = 1.0\n",
    "  Y.append(temp_vec)\n",
    "\n",
    "test_data = [p for p in zip(X, Y)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pa-cVgqyk5uO"
   },
   "source": [
    "#### We introduce three types of activation functions: sigmoid, tanh, and Relu \n",
    "* A sigmoid function have a characteristic \"S\"-shaped curve or sigmoid curve. And its output values are constrained between 0 and 1. When the z value equal 0, the output value will equal to 0.5.\n",
    "<table><tr>\n",
    "<td> <img src=\"https://miro.medium.com/max/700/1*a04iKNbchayCAJ7-0QlesA.png\" width=\"500\"/> </td>\n",
    "<td> <img src=\"https://i.stack.imgur.com/ulslG.png\" width=\"200\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "* Hyperbolic tangent activation function is also referred to as the Tanh. It is very similar to the sigmoid activation function and even has the same S-shape.\n",
    "The function takes any real value as input and outputs values in the range -1 to 1. The larger the input (more positive), the closer the output value will be to 1.0, whereas the smaller the input (more negative), the closer the output will be to -1.0.\n",
    "<table><tr>\n",
    "<td> <img src=\"https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-27_at_4.23.22_PM_dcuMBJl.png\" width=\"400\"/> </td>\n",
    "<td> <img src=\"https://64.media.tumblr.com/3025e3dbafdb688bf0ca951628be912e/tumblr_inline_nt2ci2P4Wa1qzd4wm_400.gifv\" width=\"300\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "* The rectified linear activation function (ReLU) is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero. It has become the default activation function for many types of neural networks because a model that uses it is easier to train and often achieves better performance.\n",
    "<table><tr>\n",
    "<td> <img src=\"https://miro.medium.com/max/357/1*oePAhrm74RNnNEolprmTaQ.png\" width=\"400\"/> </td>\n",
    "<td> <img src=\"https://sebastianraschka.com/images/faq/relu-derivative/deriv-7.png\" width=\"200\"/> </td>\n",
    "</tr></table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we build a function for our activation function sigmoid, and its derivative. Also build a function for the loss function MSE. sigoind_prime is the first derivative of sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "uQ2RvMV5qJsu"
   },
   "outputs": [],
   "source": [
    "# now we build a function for our activation function sigmoid, and its derivative. Also build a function for the loss function MSE. sigoind_prime is the first derivative of sigmoid function\n",
    "def sigmoid(z):\n",
    "  return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "  return sigmoid(z)*(1.0-sigmoid(z))\n",
    "\n",
    "\n",
    "def mse(a, y):\n",
    "  return .5*sum((a[i]-y[i])**2 for i in range(10))[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next we setup the initialized weights for the layers. Because we have 784 input data in the first layer, we have 784 neurons in the input layer.Also we have 60 nodes in the two hidden layers, and since we have 10 classes, the last layer will need to have 10 nodes. The number of nodes in the hidden layers can be whatever you want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "c7iB08AIrCbc"
   },
   "outputs": [],
   "source": [
    "#next we setup the initialized weights for the layers. Because we have 784 input data in the first layer, we have 784 neurons in the input layer.Also we have 60 nodes in the two hidden layers, and since we have 10 classes, the last layer will need to have 10 nodes. The number of nodes in the hidden layers can be whatever you want. \n",
    "def initialize_weights(layers = [784, 60, 60, 10]):\n",
    "  W = [[0.0]] #W = [[0,0],W1,W2,W3], placeholder\n",
    "  B = [[0.0]]\n",
    "  for i in range(1, len(layers)):\n",
    "    w_temp = np.random.randn(layers[i], layers[i-1])*np.sqrt(2/layers[i-1]) \n",
    "    b_temp = np.random.randn(layers[i], 1)*np.sqrt(2/layers[i-1])\n",
    "\n",
    "    W.append(w_temp)\n",
    "    B.append(b_temp)\n",
    "  return W, B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "store W and B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "2NrwX_wRrEx8"
   },
   "outputs": [],
   "source": [
    "W, B = initialize_weights() #store W and B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "m1Xn3ZXJrN57"
   },
   "outputs": [],
   "source": [
    "x, y = train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set a0 as our original input data, z1 as linear combination of the weight and input, and a1 as the z1 value after activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "G1cBt3SotSqZ"
   },
   "outputs": [],
   "source": [
    "# set a0 as our original input data, z1 as linear combination of the weight and input, and a1 as the z1 value after activation function\n",
    "a0 = x\n",
    "z1 = (W[1] @ a0) + B[1]\n",
    "a1 = sigmoid(z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6dflUcRI-BpL",
    "outputId": "d99338a9-caf1-4a1f-bfbb-558eb0fc75d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 1)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so a1 will become the new input for the next layer, and we repeat it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "89DswyFrtiT5",
    "outputId": "866880be-8ac3-4a2a-d534-d20b4b5e063e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 1)\n"
     ]
    }
   ],
   "source": [
    "#so a1 will become the new input for the next layer, and we repeat it again\n",
    "z2 = (W[2] @ a1) + B[2]\n",
    "a2 = sigmoid(z2)\n",
    "print(a2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "same thing for the next layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7uB0Mk4otizq",
    "outputId": "4a3c08ad-47fd-43e7-9cd0-c61b305ef66f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "#same thing for the next layer\n",
    "z3 = (W[3] @ a2) + B[3]\n",
    "a3 = sigmoid(z3)\n",
    "print(a3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now put everything together and feed forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "HdrA_7UOt5DX"
   },
   "outputs": [],
   "source": [
    "#now put everything together and feed forward\n",
    "W, B = initialize_weights(layers=[784, 60, 60, 10])\n",
    "x, y = train_data[0]\n",
    "Z = [[0.0]]\n",
    "A = [x]\n",
    "L = len(B)\n",
    "for i in range(1, L):\n",
    "  z = (W[i] @ A[i-1]) + B[i]\n",
    "  a = sigmoid(z)\n",
    "\n",
    "  Z.append(z)\n",
    "  A.append(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the shape for output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qb0DO1EhurnG",
    "outputId": "9053d845-4a40-4898-d811-9fb89919606e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[-1].shape #output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we build a dictionary to stores the delta values for backporp\n",
    "we first calculate the delta for the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "3jAcb248uswW"
   },
   "outputs": [],
   "source": [
    "deltas = dict()\n",
    "delta_last = (A[-1] - y)*sigmoid_prime(Z[-1])\n",
    "deltas[L-1] = delta_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UZ7zYBAHBWDd",
    "outputId": "f373fc19-ea4a-4346-efcd-9b030e18d490"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.14413905],\n",
       "       [ 0.14524268],\n",
       "       [ 0.14662406],\n",
       "       [ 0.1318953 ],\n",
       "       [ 0.12228555],\n",
       "       [ 0.14696222],\n",
       "       [ 0.09010994],\n",
       "       [ 0.1480463 ],\n",
       "       [ 0.04486886],\n",
       "       [-0.14736523]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltas[L-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "delta 1 and 2 for the second and third layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "3M5rIEebvg90"
   },
   "outputs": [],
   "source": [
    "for l in range(L-2, 0, -1):\n",
    "  deltas[l] = (W[l+1].T @ deltas[l+1])*sigmoid_prime(Z[l]) #has delta3 in the previous line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the delta key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h6emQOjiBcwt",
    "outputId": "7058b53b-857d-4b15-e280-ef77872cdb6d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([3, 2, 1])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltas.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "second layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ms6IMLOIvhqW",
    "outputId": "e472aae2-f1a7-4550-fa9b-952c8859d413"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 1)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltas[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "third layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B3nI8EqkYzZi",
    "outputId": "09748369-190c-4750-ebc1-010874765a8e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 1)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltas[2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QrrmclrGYznx",
    "outputId": "0e28adf8-9ed4-4427-bd6f-92054d758e58"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltas[3].shape #bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "wDGqFoFQwTqE"
   },
   "outputs": [],
   "source": [
    "alpha = 0.04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "HKCHpDeVwgtj"
   },
   "outputs": [],
   "source": [
    "for i in range(1, 4):\n",
    "  W[i] = W[i] - alpha*deltas[i]@A[i-1].T\n",
    "  B[i] = B[i] - alpha*deltas[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we put everything together, feed forward, prediction, delta, backrpop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "K8mHDPhiwxC0"
   },
   "outputs": [],
   "source": [
    "def forward_pass(W, B, p, predict_vector = False):\n",
    "  Z =[[0.0]]\n",
    "  A = [p[0]]\n",
    "  L = len(W)\n",
    "  for i in range(1, L):\n",
    "    z = (W[i] @ A[i-1]) + B[i]\n",
    "    a = sigmoid(z)\n",
    "\n",
    "    Z.append(z)\n",
    "    A.append(a)\n",
    "\n",
    "  if predict_vector == True:\n",
    "    return A[-1]\n",
    "  else:\n",
    "    return Z, A\n",
    "\n",
    "def deltas_dict(W, B, p):\n",
    "  Z, A = forward_pass(W, B, p)\n",
    "  L = len(W)\n",
    "  deltas = dict()\n",
    "  deltas[L-1] = (A[-1] - p[1])*sigmoid_prime(Z[-1])\n",
    "  for l in range(L-2, 0, -1):\n",
    "    deltas[l] = (W[l+1].T @ deltas[l+1]) * sigmoid_prime(Z[l])\n",
    "\n",
    "  return A, deltas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss function with feed forward prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "nayiEoIVyepQ"
   },
   "outputs": [],
   "source": [
    "def MSE(W, B, data):\n",
    "  c = 0.0\n",
    "  for p in data:\n",
    "    a = forward_pass(W, B, p, predict_vector=True)\n",
    "    c += mse(a, p[1])\n",
    "  return c/len(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate the initial cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jF7uMRvUzO3w",
    "outputId": "aa241b92-1c42-4a98-cb7f-d971ba9afb05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost = 1.0357163740628248\n"
     ]
    }
   ],
   "source": [
    "W, B = initialize_weights()\n",
    "print(f\"Initial Cost = {MSE(W, B, train_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see if the prediction match the true label. It is reasonable that it did poorly because it has not been trained (backpop and gradient descent) yet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "jioHLhRrzcow",
    "outputId": "46913253-3b62-45e3-f5c5-4ec11a051204"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label = 2\n",
      "Actual label = 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = np.random.randint(0, len(test_X))\n",
    "prediction = np.argmax(forward_pass(W, B, test_data[i], predict_vector=True))\n",
    "print(f\"Predicted label = {prediction}\")\n",
    "print(f\"Actual label = {test_y[i]}\")\n",
    "plt.imshow(test_X[i], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we can build a gradient descent with epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "F-Ggl8gj0VW-"
   },
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(W, B, data, alpha = 0.04, epochs = 3):\n",
    "  L = len(W)\n",
    "  print(f\"Initial Cost = {MSE(W, B, data)}\")\n",
    "  for k in range(epochs):\n",
    "    for p in data:\n",
    "      A, deltas = deltas_dict(W, B, p)\n",
    "      for i in range(1, L):\n",
    "        W[i] = W[i] - alpha*deltas[i]@A[i-1].T\n",
    "        B[i] = B[i] - alpha*deltas[i]\n",
    "    print(f\"{k} Cost = {MSE(W, B, data)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since we have epoch =3, we expect the whole data will be trained 3 times and so have 3 costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qb-6wXqIa-bv",
    "outputId": "00d4b42d-c7d9-4d59-98a0-7f5a174f5888"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost = 1.0357163740628248\n",
      "0 Cost = 0.12253924643183176\n",
      "1 Cost = 0.10807919912428347\n",
      "2 Cost = 0.09973530762867618\n"
     ]
    }
   ],
   "source": [
    "stochastic_gradient_descent(W,B,train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we now put everything together in a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "WCsZvETL0fk-"
   },
   "outputs": [],
   "source": [
    "class MultilayerPerceptron():\n",
    "  \n",
    "  def __init__(self, layers = [784, 60, 60, 10]):\n",
    "    self.layers = layers\n",
    "    self.L = len(self.layers)\n",
    "    self.W =[[0.0]]\n",
    "    self.B = [[0.0]]\n",
    "    for i in range(1, self.L):\n",
    "      w_temp = np.random.randn(self.layers[i], self.layers[i-1])*np.sqrt(2/self.layers[i-1])\n",
    "      b_temp = np.random.randn(self.layers[i], 1)*np.sqrt(2/self.layers[i-1])\n",
    "\n",
    "      self.W.append(w_temp)\n",
    "      self.B.append(b_temp)\n",
    "\n",
    "  def reset_weights(self, layers = [784, 60, 60, 10]):\n",
    "    self.layers = layers\n",
    "    self.L = len(self.layers)\n",
    "    self.W = [[0.0]]\n",
    "    self.B = [[0.0]]\n",
    "    for i in range(1, self.L):\n",
    "      w_temp = np.random.randn(self.layers[i], self.layers[i-1])*np.sqrt(2/self.layers[i-1])\n",
    "      b_temp = np.random.randn(self.layers[i], 1)*np.sqrt(2/self.layers[i-1])\n",
    "\n",
    "      self.W.append(w_temp)\n",
    "      self.B.append(b_temp)\n",
    "\n",
    "\n",
    "  def forward_pass(self, p, predict_vector = False):\n",
    "    Z =[[0.0]]\n",
    "    A = [p[0]]\n",
    "    for i in range(1, self.L):\n",
    "      z = (self.W[i] @ A[i-1]) + self.B[i]\n",
    "      a = sigmoid(z)\n",
    "      Z.append(z)\n",
    "      A.append(a)\n",
    "\n",
    "    if predict_vector == True:\n",
    "      return A[-1]\n",
    "    else:\n",
    "      return Z, A\n",
    "\n",
    "  def MSE(self, data):\n",
    "    c = 0.0\n",
    "    for p in data:\n",
    "      a = self.forward_pass(p, predict_vector=True)\n",
    "      c += mse(a, p[1])\n",
    "    return c/len(data)\n",
    "\n",
    "  def deltas_dict(self, p):\n",
    "    Z, A = self.forward_pass(p)\n",
    "    deltas = dict()\n",
    "    deltas[self.L-1] = (A[-1] - p[1])*sigmoid_prime(Z[-1])\n",
    "    for l in range(self.L-2, 0, -1):\n",
    "      deltas[l] = (self.W[l+1].T @ deltas[l+1]) * sigmoid_prime(Z[l])\n",
    "\n",
    "    return A, deltas\n",
    "\n",
    "\n",
    "  def stochastic_gradient_descent(self, data, alpha = 0.04, epochs = 3):\n",
    "    print(f\"Initial Cost = {self.MSE(data)}\")\n",
    "    for k in range(epochs):\n",
    "      for p in data:\n",
    "        A, deltas = self.deltas_dict(p)\n",
    "        for i in range(1, self.L):\n",
    "          self.W[i] = self.W[i] - alpha*deltas[i]@A[i-1].T\n",
    "          self.B[i] = self.B[i] - alpha*deltas[i]\n",
    "    print(f\"{k} Cost = {self.MSE(data)}\")\n",
    "\n",
    "  def mini_batch_gradient_descent(self, data, batch_size =15, alpha=0.04, epochs=3):\n",
    "    print(f\"Initial Cost = {self.MSE(data)}\")\n",
    "    data_length=len(data)\n",
    "    for k in range(epochs):\n",
    "      for j in range(0,data_length-batch_size,batch_size):\n",
    "        delta_list = []\n",
    "        A_list = []\n",
    "        for p in data[j:j+batch_size]:\n",
    "          A, deltas = self.deltas_dict(p)\n",
    "          A_list.append(A)\n",
    "\n",
    "          for i in range(1, self.L):\n",
    "            self.W[i] = self.W[i] - (alpha/batch_size)*sum(da[0][i]@da[1][i-1].T for da in zip(delta_list, A_list))\n",
    "            self.B[i] = self.B[i] - (alpha/batch_size)*sum(deltas[i] for deltas in delta_list)\n",
    "    print(f\"{k} Cost = {self.MSE(data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "JHxk-iGLHzEG"
   },
   "outputs": [],
   "source": [
    "net = MultilayerPerceptron(layers=[784, 60, 60, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8wSPs133H_HT",
    "outputId": "9a58b833-10b4-4731-d8f7-a309572659b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost = 1.0870015488769205\n",
      "2 Cost = 0.10102098319027059\n"
     ]
    }
   ],
   "source": [
    "net.stochastic_gradient_descent(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mini batch method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eJVa0rfEIBXj",
    "outputId": "e525b031-5b25-4223-e33f-609548232f28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost = 0.10102098319027059\n",
      "2 Cost = 0.10102098319027059\n"
     ]
    }
   ],
   "source": [
    "net.mini_batch_gradient_descent(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check testing error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p1nRSC4NlSku",
    "outputId": "63f650a3-4c0c-4189-b2d0-c624ab29ff89"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1124604508278882"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.MSE(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzISPmIsjiCD"
   },
   "source": [
    "### Add different activation function in the network (add tanh, relu and sigmoid in the function.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "adeRyWwXKMP2"
   },
   "outputs": [],
   "source": [
    "#add tanh, relu and sigmoid in the function.\n",
    "class MultilayerPerceptron():\n",
    "  \n",
    "  def __init__(self, layers = [784, 60, 60, 10],actFun_type='tanh'):\n",
    "    self.actFun_type = actFun_type\n",
    "    self.layers = layers\n",
    "    self.L = len(self.layers)\n",
    "    self.W =[[0.0]]\n",
    "    self.B = [[0.0]]\n",
    "    for i in range(1, self.L):\n",
    "      w_temp = np.random.randn(self.layers[i], self.layers[i-1])*np.sqrt(2/self.layers[i-1])\n",
    "      b_temp = np.random.randn(self.layers[i], 1)*np.sqrt(2/self.layers[i-1])\n",
    "\n",
    "      self.W.append(w_temp)\n",
    "      self.B.append(b_temp)\n",
    "\n",
    "  def actFun(self, z, type):\n",
    "\n",
    "      if type == 'tanh':\n",
    "          return np.tanh(z)\n",
    "      elif type == 'sigmoid':\n",
    "          return 1.0 / (1.0 + np.exp(-z))\n",
    "      elif type == 'relu':\n",
    "          return np.maximum(0, z)\n",
    "      else:\n",
    "          return None\n",
    "\n",
    "  def diff_actFun(self, z, type):\n",
    "\n",
    "          # YOU IMPLEMENT YOUR diff_actFun HERE\n",
    "      if type == 'tanh':\n",
    "          return 1.0 - (np.tanh(z))**2\n",
    "      elif type == 'sigmoid':\n",
    "          return np.exp(z)/(1.0+np.exp(z))**2\n",
    "      elif type == 'relu':\n",
    "          return np.where(z > 0, 1.0, 0)\n",
    "      else:\n",
    "          return None\n",
    "\n",
    "  def reset_weights(self, layers = [784, 60, 60, 10]):\n",
    "    self.layers = layers\n",
    "    self.L = len(self.layers)\n",
    "    self.W = [[0.0]]\n",
    "    self.B = [[0.0]]\n",
    "    for i in range(1, self.L):\n",
    "      w_temp = np.random.randn(self.layers[i], self.layers[i-1])*np.sqrt(2/self.layers[i-1])\n",
    "      b_temp = np.random.randn(self.layers[i], 1)*np.sqrt(2/self.layers[i-1])\n",
    "\n",
    "      self.W.append(w_temp)\n",
    "      self.B.append(b_temp)\n",
    "\n",
    "\n",
    "  def forward_pass(self, p, predict_vector = False):\n",
    "    Z =[[0.0]]\n",
    "    A = [p[0]]\n",
    "    for i in range(1, self.L):\n",
    "      z = (self.W[i] @ A[i-1]) + self.B[i]\n",
    "      a = self.actFun(z, self.actFun_type) #############\n",
    "      Z.append(z)\n",
    "      A.append(a)\n",
    "\n",
    "    if predict_vector == True:\n",
    "      return A[-1]\n",
    "    else:\n",
    "      return Z, A\n",
    "\n",
    "  def MSE(self, data):\n",
    "    c = 0.0\n",
    "    for p in data:\n",
    "      a = self.forward_pass(p, predict_vector=True)\n",
    "      c += mse(a, p[1])\n",
    "    return c/len(data)\n",
    "\n",
    "  def deltas_dict(self, p):\n",
    "    Z, A = self.forward_pass(p)\n",
    "    deltas = dict()\n",
    "    deltas[self.L-1] = (A[-1] - p[1])*self.diff_actFun(Z[-1], self.actFun_type)\n",
    "    for l in range(self.L-2, 0, -1):\n",
    "      deltas[l] = (self.W[l+1].T @ deltas[l+1]) *self.diff_actFun(Z[l], self.actFun_type)\n",
    "\n",
    "    return A, deltas\n",
    "\n",
    "\n",
    "  def stochastic_gradient_descent(self, data, alpha = 0.04, epochs = 3):\n",
    "    print(f\"Initial Cost = {self.MSE(data)}\")\n",
    "    for k in range(epochs):\n",
    "      for p in data:\n",
    "        A, deltas = self.deltas_dict(p)\n",
    "        for i in range(1, self.L):\n",
    "          self.W[i] = self.W[i] - alpha*deltas[i]@A[i-1].T\n",
    "          self.B[i] = self.B[i] - alpha*deltas[i]\n",
    "    print(f\"{k} Cost = {self.MSE(data)}\")\n",
    "\n",
    "  def mini_batch_gradient_descent(self, data, batch_size =15, alpha=0.04, epochs=3):\n",
    "    print(f\"Initial Cost = {self.MSE(data)}\")\n",
    "    data_length=len(data)\n",
    "    for k in range(epochs):\n",
    "      for j in range(0,data_length-batch_size,batch_size):\n",
    "        delta_list = []\n",
    "        A_list = []\n",
    "        for p in data[j:j+batch_size]:\n",
    "          A, deltas = self.deltas_dict(p)\n",
    "          A_list.append(A)\n",
    "\n",
    "          for i in range(1, self.L):\n",
    "            self.W[i] = self.W[i] - (alpha/batch_size)*sum(da[0][i]@da[1][i-1].T for da in zip(delta_list, A_list))\n",
    "            self.B[i] = self.B[i] - (alpha/batch_size)*sum(deltas[i] for deltas in delta_list)\n",
    "    print(f\"{k} Cost = {self.MSE(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNTqPXL2kcmJ"
   },
   "source": [
    "Activation: Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7dl3x6dwMZy8",
    "outputId": "f2ec88d6-aa67-419a-9f28-f280b4a6cf12"
   },
   "outputs": [],
   "source": [
    "net_tanh = MultilayerPerceptron(layers=[784, 60, 60, 10],actFun_type='tanh')\n",
    "# net_tanh.stochastic_gradient_descent(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "f0nabFyMk3uW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost = 2.0219797056486137\n",
      "2 Cost = 2.0219797056486137\n"
     ]
    }
   ],
   "source": [
    "net_tanh.mini_batch_gradient_descent(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GK_Y5br2P14d",
    "outputId": "68050efc-b434-4b22-8ac2-4322632cb236"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0226435315378692"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_tanh.MSE(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iqsl0bWwkhKN"
   },
   "source": [
    "Activation: Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aJt6nk64NODU",
    "outputId": "abde5420-a590-4d0c-aad4-828c3fe15ee2"
   },
   "outputs": [],
   "source": [
    "net_relu = MultilayerPerceptron(layers=[784, 100, 100, 10],actFun_type='relu')\n",
    "# net_relu.stochastic_gradient_descent(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "DvYFYmJ6k-l3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost = 1.677351514619462\n",
      "2 Cost = 1.677351514619462\n"
     ]
    }
   ],
   "source": [
    "net_relu.mini_batch_gradient_descent(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L6SIlH29P94e",
    "outputId": "26bea6f2-4dc7-4340-a130-5cbdc8dc20de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6717335353234182"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_relu.MSE(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxIjwV8UkmLk"
   },
   "source": [
    "Activation: Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rMHhI4aFPgjn",
    "outputId": "dd648d22-cbda-4520-ff86-c9e3ba81284b"
   },
   "outputs": [],
   "source": [
    "net_sig = MultilayerPerceptron(layers=[784, 100, 100, 10],actFun_type='sigmoid')\n",
    "# net_sig.stochastic_gradient_descent(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "R3UKEA45lCI8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost = 1.2531857555833366\n",
      "2 Cost = 1.2531857555833366\n"
     ]
    }
   ],
   "source": [
    "net_sig.mini_batch_gradient_descent(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7y-aS3E6P_QC",
    "outputId": "3160618c-3518-41fa-cd35-aa51b58c213f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2533420620113405"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_sig.MSE(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtyiAm9bjeA-"
   },
   "source": [
    "### Conclusion\n",
    "In this example, sigmoid activation perfomed the best in term of test error."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MLP_Good_my_version.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
